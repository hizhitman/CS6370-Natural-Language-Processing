{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "loaded_matrix = np.load(\"/content/precision_matrix (1).npy\")\n",
        "lsa_matrix=np.load(\"/content/precision_matrix_lsa.npy\")\n",
        "crn_matrix=np.load(\"/content/precision_matrix_crn.npy\")\n",
        "bert_matrix=np.load(\"/content/precision_matrix_bert.npy\")\n",
        "bm25_matrix=np.load(\"/content/precision_matrix_bm25.npy\")\n",
        "d2v_matrix=np.load(\"/content/precision_matrix_d2v_pdbow.npy\")\n",
        "w2v_matrix=np.load(\"/content/precision_matrix_w2v.npy\")"
      ],
      "metadata": {
        "id": "xoMWyCMSKt7h"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(lsa_matrix[:,1:].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21cK1oiZzqXh",
        "outputId": "48682906-0410-4fa5-dd4a-4773458b94b5"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(225, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Sample data format (replace with your actual metrics)\n",
        "# Columns: query_id, vector_map, crn_map, vector_ndcg, crn_ndcg, ...\n",
        "\n",
        "# Configuration\n",
        "METRICS = ['map']  # Add other metrics as needed\n",
        "ALPHA = 0.05\n",
        "N_COMPARISONS = len(METRICS)  # For Bonferroni correction\n",
        "\n",
        "def compare_models(k,METRICS):\n",
        "    results = []\n",
        "\n",
        "    for metric in METRICS:\n",
        "        vector_scores = loaded_matrix[:,k]\n",
        "        algo_scores = d2v_matrix[:,k]\n",
        "\n",
        "        # Calculate differences\n",
        "        differences = algo_scores - vector_scores\n",
        "\n",
        "        # Normality check\n",
        "        _, p_normal = stats.shapiro(differences)\n",
        "\n",
        "        # Hypothesis testing\n",
        "        if p_normal > 0.05:\n",
        "            # Use paired t-test if differences are normal\n",
        "            t_stat, p_value = stats.ttest_rel(vector_scores, algo_scores)\n",
        "            test_used = \"Paired t-test\"\n",
        "        else:\n",
        "            # Use Wilcoxon signed-rank test for non-normal data\n",
        "            stat, p_value = stats.wilcoxon(vector_scores, algo_scores)\n",
        "            test_used = \"Wilcoxon signed-rank\"\n",
        "\n",
        "        # Effect size calculation (Cohen's d)\n",
        "        mean_diff = np.mean(differences)\n",
        "        std_diff = np.std(differences, ddof=1)\n",
        "        cohens_d = mean_diff / std_diff if std_diff != 0 else 0\n",
        "\n",
        "        results.append({\n",
        "            'metric': metric.upper(),\n",
        "            'test_used': test_used,\n",
        "            'p_value': p_value,\n",
        "            'cohens_d': cohens_d,\n",
        "            'vector_mean': np.mean(vector_scores),\n",
        "            'algo_mean': np.mean(algo_scores)\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "print(\"d2v vs vsm\") #Change algo name\n",
        "# Run comparison\n",
        "k=[1,2,3,4,5,6,7,8,9,10]\n",
        "for i in range(10):\n",
        "  print(\"K equals : \",k[i])\n",
        "  results_df = compare_models(k[i],METRICS)\n",
        "\n",
        "\n",
        "# Display results\n",
        "  print(\"Statistical Comparison Results:\")\n",
        "  print(results_df[['metric', 'test_used', 'p_value', 'cohens_d']])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yV9RiaP22Ky4",
        "outputId": "a95b2833-4bb0-4aae-f50f-b92c8513f851"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "d2v vs vsm\n",
            "K equals :  1\n",
            "Statistical Comparison Results:\n",
            "  metric             test_used   p_value  cohens_d\n",
            "0    MAP  Wilcoxon signed-rank  0.000072  -0.27378\n",
            "K equals :  2\n",
            "Statistical Comparison Results:\n",
            "  metric             test_used   p_value  cohens_d\n",
            "0    MAP  Wilcoxon signed-rank  0.000032 -0.281312\n",
            "K equals :  3\n",
            "Statistical Comparison Results:\n",
            "  metric             test_used   p_value  cohens_d\n",
            "0    MAP  Wilcoxon signed-rank  0.000046 -0.295702\n",
            "K equals :  4\n",
            "Statistical Comparison Results:\n",
            "  metric             test_used   p_value  cohens_d\n",
            "0    MAP  Wilcoxon signed-rank  0.000039  -0.28703\n",
            "K equals :  5\n",
            "Statistical Comparison Results:\n",
            "  metric             test_used   p_value  cohens_d\n",
            "0    MAP  Wilcoxon signed-rank  0.000021 -0.301212\n",
            "K equals :  6\n",
            "Statistical Comparison Results:\n",
            "  metric             test_used   p_value  cohens_d\n",
            "0    MAP  Wilcoxon signed-rank  0.000042 -0.290317\n",
            "K equals :  7\n",
            "Statistical Comparison Results:\n",
            "  metric             test_used   p_value  cohens_d\n",
            "0    MAP  Wilcoxon signed-rank  0.000021 -0.293458\n",
            "K equals :  8\n",
            "Statistical Comparison Results:\n",
            "  metric             test_used  p_value  cohens_d\n",
            "0    MAP  Wilcoxon signed-rank  0.00001 -0.309151\n",
            "K equals :  9\n",
            "Statistical Comparison Results:\n",
            "  metric             test_used   p_value  cohens_d\n",
            "0    MAP  Wilcoxon signed-rank  0.000006  -0.31615\n",
            "K equals :  10\n",
            "Statistical Comparison Results:\n",
            "  metric             test_used   p_value  cohens_d\n",
            "0    MAP  Wilcoxon signed-rank  0.000003 -0.326984\n"
          ]
        }
      ]
    }
  ]
}